# -*- coding: utf-8 -*-
"""
Created on Sun Mar 16 12:59:05 2025

@author: user
@studentID: 5885221
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.linalg import expm
from sympy import symbols
from math import isclose
import cvxpy as cp
from collections import Counter
import scipy.io
import math
import time
np.random.seed(19680806)


""" Variables """
change_Dataset = scipy.io.loadmat('change_detection.mat')
a = np.array((change_Dataset['a']))
b = np.array((change_Dataset['b']))
c = np.array((change_Dataset['c']))
y = np.array((change_Dataset['y'])).reshape(-1, 1)
N = len(y)
variance = 0.5
v = np.random.normal(0, variance**2, (N, 1))
x = np.zeros((3, N))
w_true = np.vstack((a.T, b.T, c.T))   # shape (3, N)
w_single = np.zeros((3, N))
w_CVX = np.zeros((3, N))
w_gradientDescent = np.zeros((3, N))
w_prev = np.zeros((3, 1))
iterMax = 30000
eps = 1e-8

# optimized values of a,b,c assuming no infrequent switching, not solvable cause 3 unknowns and 1 equation:
""""
for i in range(6,N-3):
    x[i,:] = np.array(([y[i-1],y[i-2],y[i-3]]))
    test = x[i,:].T@x[i,:]
    print(test)
    print(x[i])
    w[i] = np.linalg.inv(x[i]@x[i].T)@x[i].T*y[i]
    print(w[i])
"""

""" Functions """
def convexSolve_all(x,y,N,Lapda_value):
    #x /= (np.linalg.norm(x, axis=0, keepdims=True) + eps)
    W = cp.Variable((3,N-3))
    Lapda = cp.Parameter(nonneg=True)
    # should be tested with different lapda values!
    Lapda.value = Lapda_value
    const = cp.Constant(0)
    obj = cp.Constant(0)
    # shapes:
    # X is 3,N
    # y is N,1
    # W is 3,N

    for i in range(3,N):
        obj += (cp.square(y[i]-x[:,i].T@W[:,i-3]))
        if i>4:
            obj += Lapda*(cp.norm(W[:,i-3]-W[:,i-4],2))
            const += (cp.norm(W[:,i-3]-W[:,i-4],1))
    constraints = [const<=3+eps]
    prob = cp.Problem(cp.Minimize(obj), constraints)
    prob.solve(solver=cp.SCS, verbose=True)
    return W.value,prob.solver_stats.num_iters

def objective(W, x, y, Lapda):
    obj = 0
    for i in range(N-3):
        obj += ((y[i]-x[:,i].T@W[:,i])**2)
        if i>=1:
            obj += Lapda*(np.linalg.norm(W[:,i]-W[:,i-1],2))
    return obj

def simpleSubGradientDescent(x,y,N,Lapda,Alpha0=1e-1, tol=5e-7):
    # need the dual function?
    ## Not needed unless there is a distributed problem!
    # need to find subgradient, this is needed due to the norm in the objective function:
    ## comes from ((y[i]-x[:,i].T@W[:,i])**2)'
    x_trim = x[:, 3:]   # 3 x (N-3)
    y_trim = y[3:]      # length (N-3)
    x_norm = x_trim / (np.linalg.norm(x_trim, axis=0, keepdims=True)+eps) # normalize
    W_out = np.zeros((3, N-3))
    W_simple = np.zeros_like(W_out)
    #initial condition?
    #W_out[:,0] = np.array(([1,-0.2,-0.8])) # something nonzero
    prev_obj = 1000
    k_out = 0
    for k in range(iterMax):
        subgradient = np.zeros_like(W_out)

    # data term
        for j in range(N-3):
            r = x_trim[:,j].T @ W_out[:,j] - y_trim[j]
            subgradient[:,j] += 2 * x_trim[:,j] * r

    # TV term
        for j in range(N-4):
            diff = W_out[:,j+1] - W_out[:,j]
            norm = np.linalg.norm(diff)
            if norm > eps:
                g = Lapda * diff / norm
                subgradient[:,j+1] += g
                subgradient[:,j]   -= g

        obj_W = objective(W_out,x_trim,y_trim,Lapda)
        objective_value = max(obj_W,0.0)
        #Alpha_value = Alpha0 / np.sqrt(k + 1) # simple decaying stepsize
        Alpha_value = objective_value/((np.linalg.norm(subgradient)**2)+eps)
        W_out -= Alpha_value * subgradient

        rel_change = abs(obj_W - prev_obj) / max(1.0, abs(prev_obj))
        if rel_change < tol:
            print("convergence at: ",k)
            k_out=k
            break
        prev_obj = obj_W
    return W_out,k_out

def project_l1_ball(v, C):
    """
    Project vector v onto {||v||_1 <= C}
    """
    if np.sum(np.abs(v)) <= C:
        return v

    u = np.sort(np.abs(v))[::-1]
    sv = np.cumsum(u)
    rho = np.where(u * np.arange(1, len(u)+1) > (sv - C))[0][-1]
    theta = (sv[rho] - C) / (rho + 1)
    return np.sign(v) * np.maximum(np.abs(v) - theta, 0)

def project_total_change(W, C):
    D = np.diff(W, axis=1)
    D_proj = project_l1_ball(D.flatten(), C)
    D_proj = D_proj.reshape(D.shape)

    W_proj = np.zeros_like(W)
    W_proj[:,0] = W[:,0]
    for t in range(1, W.shape[1]):
        W_proj[:,t] = W_proj[:,t-1] + D_proj[:,t-1]

    return W_proj

def subGradientDescent(x,y,N,Lapda, Alpha0=1e-1, tol=5e-7, C = 30):
    x_trim = x[:, 3:]   # 3 x (N-3)
    y_trim = y[3:]      # length (N-3)
    W_out = np.zeros((3, N-3))
    #initial condition?
    W_out[:,0] = np.array(([1,-0.2,-0.8])) # something nonzero
    prev_obj = 1000
    k_out = 0
    for k in range(iterMax):
        subgradient = np.zeros_like(W_out)

    # data term
        for j in range(N-3):
            r = x_trim[:,j].T @ W_out[:,j] - y_trim[j]
            subgradient[:,j] += 2 * x_trim[:,j] * r

    # TV term
        for j in range(N-4):
            diff = W_out[:,j+1] - W_out[:,j]
            norm = np.linalg.norm(diff)
            if norm > eps:
                g = Lapda * diff / norm
                subgradient[:,j+1] += g
                subgradient[:,j]   -= g

        obj_W = objective(W_out,x_trim,y_trim,Lapda)
        #objective_value = max(obj_W,0.0)
        #Alpha_value = objective_value/((np.linalg.norm(subgradient)**2)+eps)
        Alpha_value = Alpha0 / np.sqrt(k + 1)

        W_tmp = W_out - Alpha_value*subgradient
        W_out = project_total_change(W_tmp, C)

        #W_out = np.maximum(np.minimum(W_out,1.1),-1.1) # bound between -1 and 1 to improve convergence
        #W_out[2,:] = np.maximum(np.minimum(W_out[2,:],0.5),-0.5)
        # check if objective changes with new iterations of W
        rel_change = abs(obj_W - prev_obj) / max(1.0, abs(prev_obj))

        if rel_change < tol:
            print("convergence at: ",k)
            k_out=k
            break
        prev_obj = obj_W
    return W_out,k_out   
    


""" Main Code """
""" Question 1: Creation of the optimization problem """
for i in range(3,N-3):
    x_slice = np.vstack((y[i-1],y[i-2],y[i-3]))
    x[:,i] = x_slice.squeeze()
Max_Lapda = 11
best_mse1 = np.inf
lapda_space1 = np.linspace(0,1,Max_Lapda)
mse_convex = []
for l in lapda_space1:
    start1 = time.time()
    w_temp_CVX,iter_CVX = convexSolve_all(x,y,N,l)
    time_CVX = time.time() - start1
    mse_w_CVX = np.mean((w_temp_CVX - w_true)**2)
    mse_convex.append(mse_w_CVX)
    if mse_w_CVX < best_mse1:
        best_mse1 = mse_w_CVX
        bestTime_CVX = time_CVX
        bestIter_CVX = iter_CVX
        w_CVX = w_temp_CVX
        bestLapda_CVX = l

best_mse2 = np.inf
lapda_space2 = np.linspace(5,6,Max_Lapda)
mse_GD = []
for iter,l in enumerate(lapda_space2):
    print("Lapda:",l,"iter:",iter)
    start2 = time.time()
    w_temp_GD,iter_GD = simpleSubGradientDescent(x,y,N,l)
    time_GD = time.time() - start2
    mse_w_GD = np.mean((w_temp_GD - w_true)**2)
    mse_GD.append(mse_w_GD)
    if mse_w_GD < best_mse2:
        best_mse2 = mse_w_GD
        bestTime_GD = time_GD
        bestIter_GD = iter_GD
        w_gradientDescent = w_temp_GD
        bestLapda_GD = l

best_mse3 = np.inf
lapda_space3 = np.linspace(4,5,Max_Lapda)
mse_IGD = []
for iter,l in enumerate(lapda_space3):
    print("Lapda:",l,"iter:",iter)
    start3 = time.time()
    w_temp_IGD,iter_IGD = subGradientDescent(x,y,N,l)
    time_IGD =  time.time() - start3
    mse_w_IGD = np.mean((w_temp_IGD - w_true)**2)
    mse_IGD.append(mse_w_IGD)
    if mse_w_IGD < best_mse3:
        best_mse3 = mse_w_IGD
        bestTime_IGD = time_IGD
        bestIter_IGD = iter_IGD
        w_improvedGradientDescent = w_temp_IGD
        bestLapda_IGD = l

print("best value for lapda (CVX)",bestLapda_CVX)
print("best value for lapda (subgradient descent)",bestLapda_GD)
print("best value for lapda (projected subgradient descent)",bestLapda_IGD)
print("time (CVX)",bestTime_CVX)
print("time (subgradient descent)",bestTime_GD)
print("time (projected subgradient descent)",bestTime_IGD)
print("iterations (CVX)",bestIter_CVX)
print("iterations (subgradient descent)",bestIter_GD)
print("iterations (projected subgradient descent)",bestIter_IGD)
""" Plotting """

# Estimated versus true values (all):
plt.figure(figsize=(10,5))

# Estimated weights
plt.plot(w_CVX[0,:], label='Estimated a', linewidth=2, color="blue")
plt.plot(w_CVX[1,:], label='Estimated b', linewidth=2, color="red")
plt.plot(w_CVX[2,:], label='Estimated c', linewidth=2, color="green")

# True weights
plt.plot(w_true[0,:], '--', label='True a', color="blue")
plt.plot(w_true[1,:], '--', label='True b', color="red")
plt.plot(w_true[2,:], '--', label='True c', color="green")

plt.xlabel('Time index')
plt.ylabel('Coefficient value')
plt.title('Estimation of weights using CVX solver')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('CVX.png')
plt.show()

# Estimated versus true values (not CVX):
plt.figure(figsize=(10,5))

# Estimated weights
plt.plot(w_gradientDescent[0,:], label='Estimated a', linewidth=2, color="blue")
plt.plot(w_gradientDescent[1,:], label='Estimated b', linewidth=2, color="red")
plt.plot(w_gradientDescent[2,:], label='Estimated c', linewidth=2, color="green")

# True weights
plt.plot(w_true[0,:], '--', label='True a', color="blue")
plt.plot(w_true[1,:], '--', label='True b', color="red")
plt.plot(w_true[2,:], '--', label='True c', color="green")

plt.xlabel('Time index')
plt.ylabel('Coefficient value')
plt.title('Estimation of weights using subgradient descent')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('subgradientDescent.png')
plt.show()

# Estimated versus true values (not CVX):
plt.figure(figsize=(10,5))

# Estimated weights
plt.plot(w_improvedGradientDescent[0,:], label='Estimated a', linewidth=2, color="blue")
plt.plot(w_improvedGradientDescent[1,:], label='Estimated b', linewidth=2, color="red")
plt.plot(w_improvedGradientDescent[2,:], label='Estimated c', linewidth=2, color="green")

# True weights
plt.plot(w_true[0,:], '--', label='True a', color="blue")
plt.plot(w_true[1,:], '--', label='True b', color="red")
plt.plot(w_true[2,:], '--', label='True c', color="green")

plt.xlabel('Time index')
plt.ylabel('Coefficient value')
plt.title('Estimation of weights using projected subgradient descent')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('projectedSubgradientDescent.png')
plt.show()

#MSE values:
best_mse_values = [
    np.min(mse_convex),
    np.min(mse_GD),
    np.min(mse_IGD)
]

labels = [
    'Convex solver',
    'Subgradient descent',
    'Projected subgradient descent'
]

plt.figure(figsize=(7,5))
plt.bar(labels, best_mse_values)

plt.yscale('log')
plt.ylabel('Best MSE (log scale)')
plt.title('Best MSE per Algorithm')
plt.grid(axis='y', which='both', alpha=0.3)

plt.tight_layout()
plt.savefig('MSE.png')
plt.show()